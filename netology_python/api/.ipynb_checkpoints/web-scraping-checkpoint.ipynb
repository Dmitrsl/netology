{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотека requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод get\n",
    "res = requests.get('https://yandex.ru')\n",
    "res\n",
    "# res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# браузер отрисовал бы страницу на основе данного текста\n",
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим куки\n",
    "res.cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем плохой статус\n",
    "bad_request = requests.get('https://yandex.ru/secret')\n",
    "bad_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_request.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попроуем сделать post запрос\n",
    "post_req = requests.post('https://yandex.ru')\n",
    "post_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cформируем поисковый запрос, обратите внимание на его формат\n",
    "URL = 'https://yandex.ru/search/?text=python&lr=54'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(URL)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в request можно передать параметры запроса и заголовки (headers) в виде словарей. \n",
    "# сегодня не будем рассматривать примеры с необхожимостью передачи заголовка, \n",
    "# но в практике вам это точно понадобится\n",
    "URL = 'https://yandex.ru/search/'\n",
    "params = {\n",
    "    'text': 'python',\n",
    "    'lr': 54\n",
    "}\n",
    "headers = {}\n",
    "req = requests.get(URL, params)\n",
    "req = requests.post(URL, params, headers)\n",
    "req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# очень часто сайты могут ограничивать частые запросы к себе, \n",
    "# поэтому нужно задерживать исполнение\n",
    "import time\n",
    "time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как разбирать всю эту разметку? Поможет BeautifulSoup.\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика 1. Напишем скрипт, который будет отбирать посты из нужных хабов на habr.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем список хабов, которые нам интересны\n",
    "DESIRED_HUBS = ['python', 'bigdata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем страницу с самыми свежими постами\n",
    "req = requests.get('https://habr.com/ru/all/')\n",
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлекаем посты\n",
    "posts = soup.find_all('article', class_='post')\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in posts:\n",
    "    post_id = post.parent.attrs.get('id')\n",
    "   # если идентификатор не найден, это что-то странное, пропускаем\n",
    "    if not post_id:\n",
    "        continue\n",
    "    post_id = int(post_id.split('_')[-1])\n",
    "    print('post', post_id)\n",
    "    hubs = post.find_all('a', class_='hub-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем извлечение хабов из постов, чтобы отбирать только нужные\n",
    "for post in posts:\n",
    "    post_id = post.parent.attrs.get('id')\n",
    "   # если идентификатор не найден, это что-то странное, пропускаем\n",
    "    if not post_id:\n",
    "        continue\n",
    "    post_id = int(post_id.split('_')[-1])\n",
    "    hubs = post.find_all('a', class_='hub-link')\n",
    "    for hub in hubs:\n",
    "            hub_lower = hub.text.lower()\n",
    "           # ищем вхождение хотя бы одного желаемого хаба\n",
    "            if any([hub_lower in desired for desired in DESIRED_HUBS]):\n",
    "               # пост нам интересен - делаем с ним все что захотим:\n",
    "               # можно отправит в телеграм уведомление, можно на почту и т.п.\n",
    "                title_element = post.find('a', class_='post__title_link')\n",
    "                print(title_element.text, title_element.attrs.get('href'))\n",
    "               # так как пост уже нам подошел - дальше нет смысла проверять хабы\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика 2. Напишем скрипт, который будет собирать новости с сайта Коммерсанта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.kommersant.ru/search/results'\n",
    "params = {\n",
    "    'search_query': 'python'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(URL, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добираемся до блоков с новостями\n",
    "news_blocks = soup.find_all('div', class_='search_results_item')\n",
    "news_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добираемся до текста со ссылкой\n",
    "articles_intro = list(map(lambda x: x.find('div', class_='article_intro'), news_blocks))\n",
    "articles_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добираемся до ссылок\n",
    "a_list = list(map(lambda x: x.find('a').get('href'), articles_intro))\n",
    "a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем полноценные ссылки\n",
    "all_refs = list(map(lambda x: 'https://www.kommersant.ru/' + x, a_list))\n",
    "all_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединим все в одну функцию\n",
    "def get_all_links(url, query):\n",
    "    all_refs = []\n",
    "    params = {\n",
    "        'search_query': query,\n",
    "    }\n",
    "    res = requests.get(URL, params)\n",
    "    time.sleep(0.3)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    news_blocks = soup.find_all('div', class_='search_results_item')\n",
    "    articles_intro = list(map(lambda x: x.find('div', class_='article_intro'), news_blocks))\n",
    "    a_list = list(map(lambda x: x.find('a').get('href'), articles_intro))\n",
    "    all_refs = list(map(lambda x: 'https://www.kommersant.ru/' + x, a_list))\n",
    "\n",
    "    return all_refs\n",
    "\n",
    "all_links = get_all_links(URL, 'python')\n",
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# но мы же собрали только одну страницу? Хотим ВСЕ новости\n",
    "def get_all_links(url, query, pages):\n",
    "    all_refs = []\n",
    "    params = {\n",
    "        'search_query': query\n",
    "    }\n",
    "    for i in range(pages):\n",
    "        params['page'] = i + 1\n",
    "        res = requests.get(URL, params)\n",
    "        time.sleep(0.3)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        news_blocks = soup.find_all('div', class_='search_results_item')\n",
    "        articles_intro = list(map(lambda x: x.find('div', class_='article_intro'), news_blocks))\n",
    "        a_list = list(map(lambda x: x.find('a').get('href'), articles_intro))\n",
    "        all_refs += list(map(lambda x: 'https://www.kommersant.ru/' + x, a_list))\n",
    "    return all_refs\n",
    "\n",
    "all_links = get_all_links(URL, 'python', 3)\n",
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем даты, заголовки и тексты новостей\n",
    "# получаем ошибочку. Значит не у всех получаемых страниц одинаковая разметка\n",
    "for link in all_links:\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    time.sleep(0.3)\n",
    "    date = pd.to_datetime(soup.find('time', class_='title__cake').get('datetime'), dayfirst=True).date()\n",
    "    print(date)\n",
    "    title = soup.find('h1', class_='article_name').text\n",
    "    print(title)\n",
    "    text = soup.find('div', class_='article_text_wrapper').text\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем ошибочку. Значит не у всех получаемых страниц одинаковая разметка\n",
    "for link in all_links:\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    if soup.find('div', class_='b-article__publish_date'):\n",
    "        date = pd.to_datetime(soup.find('div', class_='b-article__publish_date').find('time').get('datetime'), dayfirst=True).date()\n",
    "    elif soup.find('time', class_='title__cake'):\n",
    "        date = pd.to_datetime(soup.find('time', class_='title__cake').get('datetime'), dayfirst=True).date()\n",
    "    print(date)\n",
    "    if soup.find('h2', class_='article_name'): \n",
    "        title = soup.find('h2', class_='article_name').text\n",
    "    else: \n",
    "        title = soup.find('h1', class_='article_name').text    \n",
    "    print(title)\n",
    "    text = soup.find('div', class_='article_text_wrapper').text\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запишем данные в датафрейм\n",
    "kom_news = pd.DataFrame()\n",
    "for link in all_links:\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    time.sleep(0.3)\n",
    "    if soup.find('div', class_='b-article__publish_date'):\n",
    "        date = pd.to_datetime(soup.find('div', class_='b-article__publish_date').find('time').get('datetime'), dayfirst=True).date()\n",
    "    elif soup.find('time', class_='title__cake'):\n",
    "        date = pd.to_datetime(soup.find('time', class_='title__cake').get('datetime'), dayfirst=True).date()\n",
    "    if soup.find('h2', class_='article_name'): \n",
    "        title = soup.find('h2', class_='article_name').text\n",
    "    else: \n",
    "        title = soup.find('h1', class_='article_name').text    \n",
    "    text = soup.find('div', class_='article_text_wrapper').text\n",
    "    row = {'date': date, 'title': title, 'text': text}\n",
    "    kom_news = pd.concat([kom_news, pd.DataFrame([row])])  \n",
    "kom_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обернем в функцию \n",
    "def get_kom_news(links):\n",
    "    kom_news = pd.DataFrame()\n",
    "    for link in all_links:\n",
    "        soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "        if soup.find('div', class_='b-article__publish_date'):\n",
    "            date = pd.to_datetime(soup.find('div', class_='b-article__publish_date').find('time').get('datetime'), dayfirst=True).date()\n",
    "        elif soup.find('time', class_='title__cake'):\n",
    "            date = pd.to_datetime(soup.find('time', class_='title__cake').get('datetime'), dayfirst=True).date()\n",
    "        if soup.find('h2', class_='article_name'): \n",
    "            title = soup.find('h2', class_='article_name').text\n",
    "        else: \n",
    "            title = soup.find('h1', class_='article_name').text    \n",
    "        text = soup.find('div', class_='article_text_wrapper').text\n",
    "        row = {'date': date, 'title': title, 'text': text}\n",
    "        kom_news = pd.concat([kom_news, pd.DataFrame([row])])  \n",
    "    return kom_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kom_news = get_kom_news(all_links)\n",
    "kom_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика 3. Получим данные о песнях исполнителя при помощи  [API ITunes](https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/UnderstandingSearchResults.html#//apple_ref/doc/uid/TP40017632-CH8-SW1\n",
    "URL = 'https://itunes.apple.com/search?term=jack+johnson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'term': 'шнуров',\n",
    "    'limit': 200,\n",
    "#     'offset': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', 100)\n",
    "res = requests.get(URL, params)\n",
    "# res.json()\n",
    "\n",
    "pd.DataFrame(res.json()['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'term': 'лазарев',\n",
    "    'limit': 60,\n",
    "    'attribute': 'allArtistTerm',\n",
    "    'country': 'ru'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(URL, params)\n",
    "res.json()\n",
    "\n",
    "pd.DataFrame(res.json()['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика 4. Соберем сообщения из новостной ленты ВК по нужному запросу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://vk.com/dev/manuals\n",
    "# https://vk.com/dev/newsfeed.search\n",
    "NEWSFEED_REQUEST = 'https://api.vk.com/method/newsfeed.search?'\n",
    "TOKEN = '9df7991c9df7991c9df7991c329d86910d99df79df7991cc363a27748dcf7ad91284ef6'\n",
    "VERSION = '5.103'\n",
    "SLEEP = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обращаем внимание, что максимальное количество постов, \n",
    "# которые можно вытащить за раз, ограничено\n",
    "params = {\n",
    "    'access_token': TOKEN,\n",
    "    'v': VERSION,\n",
    "    'q': 'короновирус',\n",
    "    'count': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(NEWSFEED_REQUEST, params)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()['response']['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res.json()['response']['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# соберем все доступные сообщения по запросу\n",
    "newsfeed_df = pd.DataFrame()\n",
    "while True:\n",
    "    result = requests.get(NEWSFEED_REQUEST, params)\n",
    "    time.sleep(0.33)\n",
    "    newsfeed_df = pd.concat([newsfeed_df, pd.DataFrame(result.json()['response']['items'])])\n",
    "    if 'next_from' in result.json()['response'].keys():\n",
    "        params['start_from'] = result.json()['response']['next_from']\n",
    "    else:\n",
    "        break\n",
    "newsfeed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
